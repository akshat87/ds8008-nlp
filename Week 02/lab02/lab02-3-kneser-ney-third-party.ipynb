{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one implementation of Kneser Ney, I found on Github: https://github.com/smilli/kneser-ney. I have not personally verified its correctness but it seems to work on different trigram patterns not in the existing patterns. First run the code below and then run its execution from the cell following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class KneserNeyLM:\n",
    "\n",
    "    def __init__(self, highest_order, ngrams, start_pad_symbol='<s>',\n",
    "            end_pad_symbol='</s>'):\n",
    "        \"\"\"\n",
    "        Constructor for KneserNeyLM.\n",
    "\n",
    "        Params:\n",
    "            highest_order [int] The order of the language model.\n",
    "            ngrams [list->tuple->string] Ngrams of the highest_order specified.\n",
    "                Ngrams at beginning / end of sentences should be padded.\n",
    "            start_pad_symbol [string] The symbol used to pad the beginning of\n",
    "                sentences.\n",
    "            end_pad_symbol [string] The symbol used to pad the beginning of\n",
    "                sentences.\n",
    "        \"\"\"\n",
    "        self.highest_order = highest_order\n",
    "        self.start_pad_symbol = start_pad_symbol\n",
    "        self.end_pad_symbol = end_pad_symbol\n",
    "        self.lm = self.train(ngrams)\n",
    "\n",
    "    def train(self, ngrams):\n",
    "        \"\"\"\n",
    "        Train the language model on the given ngrams.\n",
    "\n",
    "        Params:\n",
    "            ngrams [list->tuple->string] Ngrams of the highest_order specified.\n",
    "        \"\"\"\n",
    "        kgram_counts = self._calc_adj_counts(Counter(ngrams))\n",
    "        probs = self._calc_probs(kgram_counts)\n",
    "        return probs\n",
    "\n",
    "    def highest_order_probs(self):\n",
    "        return self.lm[0]\n",
    "\n",
    "    def _calc_adj_counts(self, highest_order_counts):\n",
    "        \"\"\"\n",
    "        Calculates the adjusted counts for all ngrams up to the highest order.\n",
    "\n",
    "        Params:\n",
    "            highest_order_counts [dict{tuple->string, int}] Counts of the highest\n",
    "                order ngrams.\n",
    "\n",
    "        Returns:\n",
    "            kgrams_counts [list->dict] List of dict from kgram to counts\n",
    "                where k is in descending order from highest_order to 0.\n",
    "        \"\"\"\n",
    "        kgrams_counts = [highest_order_counts]\n",
    "        for i in range(1, self.highest_order):\n",
    "            last_order = kgrams_counts[-1]\n",
    "            new_order = defaultdict(int)\n",
    "            for ngram in last_order.keys():\n",
    "                suffix = ngram[1:]\n",
    "                new_order[suffix] += 1\n",
    "            kgrams_counts.append(new_order)\n",
    "        return kgrams_counts\n",
    "\n",
    "    def _calc_probs(self, orders):\n",
    "        \"\"\"\n",
    "        Calculates interpolated probabilities of kgrams for all orders.\n",
    "        \"\"\"\n",
    "        backoffs = []\n",
    "        for order in orders[:-1]:\n",
    "            backoff = self._calc_order_backoff_probs(order)\n",
    "            backoffs.append(backoff)\n",
    "        orders[-1] = self._calc_unigram_probs(orders[-1])\n",
    "        backoffs.append(defaultdict(int))\n",
    "        self._interpolate(orders, backoffs)\n",
    "        return orders\n",
    "\n",
    "    def _calc_unigram_probs(self, unigrams):\n",
    "        sum_vals = sum(v for v in unigrams.values())\n",
    "        unigrams = dict((k, math.log(v/sum_vals)) for k, v in unigrams.items())\n",
    "        return unigrams\n",
    "\n",
    "    def _calc_order_backoff_probs(self, order):\n",
    "        num_kgrams_with_count = Counter(\n",
    "            value for value in order.values() if value <= 4)\n",
    "        discounts = self._calc_discounts(num_kgrams_with_count)\n",
    "        prefix_sums = defaultdict(int)\n",
    "        backoffs = defaultdict(int)\n",
    "        for key in order.keys():\n",
    "            prefix = key[:-1]\n",
    "            count = order[key]\n",
    "            prefix_sums[prefix] += count\n",
    "            discount = self._get_discount(discounts, count)\n",
    "            order[key] -= discount\n",
    "            backoffs[prefix] += discount\n",
    "        for key in order.keys():\n",
    "            prefix = key[:-1]\n",
    "            order[key] = math.log(order[key]/prefix_sums[prefix])\n",
    "        for prefix in backoffs.keys():\n",
    "            backoffs[prefix] = math.log(backoffs[prefix]/prefix_sums[prefix])\n",
    "        return backoffs\n",
    "\n",
    "    def _get_discount(self, discounts, count):\n",
    "        if count > 3:\n",
    "            return discounts[3]\n",
    "        return discounts[count]\n",
    "\n",
    "    def _calc_discounts(self, num_with_count):\n",
    "        \"\"\"\n",
    "        Calculate the optimal discount values for kgrams with counts 1, 2, & 3+.\n",
    "        \"\"\"\n",
    "        common = num_with_count[1]/(num_with_count[1] + 2 * num_with_count[2])\n",
    "        # Init discounts[0] to 0 so that discounts[i] is for counts of i\n",
    "        discounts = [0]\n",
    "        for i in range(1, 4):\n",
    "            if num_with_count[i] == 0:\n",
    "                discount = 0\n",
    "            else:\n",
    "                discount = (i - (i + 1) * common\n",
    "                        * num_with_count[i + 1] / num_with_count[i])\n",
    "            discounts.append(discount)\n",
    "        if any(d for d in discounts[1:] if d <= 0):\n",
    "            raise Exception(\n",
    "                '***Warning*** Non-positive discounts detected. '\n",
    "                'Your dataset is probably too small.')\n",
    "        return discounts\n",
    "\n",
    "    def _interpolate(self, orders, backoffs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        for last_order, order, backoff in zip(\n",
    "                reversed(orders), reversed(orders[:-1]), reversed(backoffs[:-1])):\n",
    "            for kgram in order.keys():\n",
    "                prefix, suffix = kgram[:-1], kgram[1:]\n",
    "                order[kgram] += last_order[suffix] + backoff[prefix]\n",
    "\n",
    "    def logprob(self, ngram):\n",
    "        for i, order in enumerate(self.lm):\n",
    "            if ngram[i:] in order:\n",
    "                return order[ngram[i:]]\n",
    "        return None\n",
    "\n",
    "    def score_sent(self, sent):\n",
    "        \"\"\"\n",
    "        Return log prob of the sentence.\n",
    "\n",
    "        Params:\n",
    "            sent [tuple->string] The words in the unpadded sentence.\n",
    "        \"\"\"\n",
    "        padded = (\n",
    "            (self.start_pad_symbol,) * (self.highest_order - 1) + sent +\n",
    "            (self.end_pad_symbol,))\n",
    "        sent_logprob = 0\n",
    "        for i in range(len(sent) - self.highest_order + 1):\n",
    "            ngram = sent[i:i+self.highest_order]\n",
    "            sent_logprob += self.logprob(ngram)\n",
    "        return sent_logprob\n",
    "\n",
    "    def generate_sentence(self, min_length=4):\n",
    "        \"\"\"\n",
    "        Generate a sentence using the probabilities in the language model.\n",
    "\n",
    "        Params:\n",
    "            min_length [int] The mimimum number of words in the sentence.\n",
    "        \"\"\"\n",
    "        sent = []\n",
    "        probs = self.highest_order_probs()\n",
    "        while len(sent) < min_length + self.highest_order:\n",
    "            sent = [self.start_pad_symbol] * (self.highest_order - 1)\n",
    "            # Append first to avoid case where start & end symbal are same\n",
    "            sent.append(self._generate_next_word(sent, probs))\n",
    "            while sent[-1] != self.end_pad_symbol:\n",
    "                sent.append(self._generate_next_word(sent, probs))\n",
    "        sent = ' '.join(sent[(self.highest_order - 1):-1])\n",
    "        return sent\n",
    "\n",
    "    def _get_context(self, sentence):\n",
    "        \"\"\"\n",
    "        Extract context to predict next word from sentence.\n",
    "\n",
    "        Params:\n",
    "            sentence [tuple->string] The words currently in sentence.\n",
    "        \"\"\"\n",
    "        return sentence[(len(sentence) - self.highest_order + 1):]\n",
    "\n",
    "    def _generate_next_word(self, sent, probs):\n",
    "        context = tuple(self._get_context(sent))\n",
    "        pos_ngrams = list(\n",
    "            (ngram, logprob) for ngram, logprob in probs.items()\n",
    "            if ngram[:-1] == context)\n",
    "        # Normalize to get conditional probability.\n",
    "        # Subtract max logprob from all logprobs to avoid underflow.\n",
    "        _, max_logprob = max(pos_ngrams, key=lambda x: x[1])\n",
    "        pos_ngrams = list(\n",
    "            (ngram, math.exp(prob - max_logprob)) for ngram, prob in pos_ngrams)\n",
    "        total_prob = sum(prob for ngram, prob in pos_ngrams)\n",
    "        pos_ngrams = list(\n",
    "            (ngram, prob/total_prob) for ngram, prob in pos_ngrams)\n",
    "        rand = random.random()\n",
    "        for ngram, prob in pos_ngrams:\n",
    "            rand -= prob\n",
    "            if rand < 0:\n",
    "                return ngram[-1]\n",
    "        return ngram[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code snippet calls the above Kneser Ney implementation to perform training and testing. To run this code you may have to download Gutenberg corpus (see below the commented cell). Try to understand the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.downlaod(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "#training\n",
    "gut_ngrams = (\n",
    "    ngram for sent in gutenberg.sents() for ngram in ngrams(sent, 3,\n",
    "    pad_left=True, pad_right=True, left_pad_symbol='<S>', right_pad_symbol='</S>' ) )\n",
    "g=list(gut_ngrams)\n",
    "print (g[:10])\n",
    "\n",
    "lm = KneserNeyLM(3, g, start_pad_symbol='<S>', end_pad_symbol='</S>')\n",
    "\n",
    "#testing\n",
    "# the code returns log probabilites not the prodcut--i.e., sum of logs (P(w3|w1,w2))\n",
    "lm.score_sent(('I', 'do', 'not', 'like', 'green', '.'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.5:\n",
    "    \n",
    "Your task is to train a new model using Laplace smoothing on Gutenberg corpus and test it on the test sentence just as used above. Note you will have to calulate log probabilites for your Laplace smoothing too, for comparison with above result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
