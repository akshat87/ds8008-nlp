{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we shall focus on language modeling using n-grams.\n",
    "\n",
    "\n",
    "Following is the code to generate n-grams with their frequencies from a text document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words::  ['Hello', 'how', 'are', 'you', 'doing?', 'Hope', 'you', 'find', 'the', 'course', 'interesting.', 'How', 'are', 'you', 'doing?']\n",
      "\n",
      "Vocabulary size::  11\n",
      "\n",
      "Top 5::   [(('are', 'you'), 2), (('you', 'doing?'), 2), (('Hello', 'how'), 1), (('how', 'are'), 1), (('doing?', 'Hope'), 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "text=u\"Hello how are you doing? Hope you find the course interesting. How are you doing?\"\n",
    "tokens=text.split() #basic tokenization\n",
    "\n",
    "## Printing vocabulary size by creating a set from tokens\n",
    "print (\"Words:: \",tokens)\n",
    "vocabulary=set(tokens)\n",
    "print(\"\\nVocabulary size:: \",len(vocabulary))\n",
    "\n",
    "#generating bi-grams from tokens\n",
    "bitokens=ngrams(tokens,2)\n",
    "\n",
    "# using a counter function to count the bigrams\n",
    "ngramsWithFreq=Counter(bitokens)\n",
    "print (\"\\nTop 5::  \", ngramsWithFreq.most_common(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an alternate mechanism to generate n-grams. We are using collocation finder class in the following code. Collocation finder helps use in finding collocated words--words appearing together.\n",
    "\n",
    "In the following code, ngram_fd is an attribute of the instance \"trigrams\" that keeps the frequency distribution of all the n-grams. ngram_fd.items() is a function that returns the list of all n-grams with their frequencies. To find about different functions and attributes in the instance \"trigrams\", type \"dir(trigrams)\" and to get the description of the listed functions use help; e.g., \"help(trigrams.ngram_fd)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello', 'how', 'are') 1\n",
      "('how', 'are', 'you') 1\n",
      "('are', 'you', 'doing') 1\n",
      "('you', 'doing', '?') 1\n",
      "('doing', '?', 'Hope') 1\n",
      "('?', 'Hope', 'you') 1\n",
      "('Hope', 'you', 'find') 1\n",
      "('you', 'find', 'the') 1\n",
      "('find', 'the', 'book') 1\n",
      "('the', 'book', 'interesting') 1\n",
      "('book', 'interesting', '.') 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "import nltk\n",
    "text=u\"Hello how are you doing? Hope you find the book interesting.\"\n",
    "\n",
    "#tokenize\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "#Using a trigram collocation finder\n",
    "trigrams=TrigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "# printing trigrams\n",
    "for trigram, freq in trigrams.ngram_fd.items():\n",
    "    print(trigram,freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we are continuing with the previous example and we are going to filter out stop words and print top few n-grams. In order to get top n-grams, we will use nbest funtion. The nbest function can return n-grams based on a scoring criterion. In this case, we are scoring by frequency, so we will use TrigramAssocMeasures class and its attribute raw_freq. There are other methods of scoring too in the TrigramAssocMeasures class, which you can find out by using help(TrigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('know', 'good', 'mentor') 1\n",
      "('good', 'mentor', 'may') 2\n",
      "('mentor', 'may', 'help') 1\n",
      "('may', 'help', '?') 1\n",
      "('help', '?', 'hope') 1\n",
      "('?', 'hope', 'find') 1\n",
      "('hope', 'find', 'book') 1\n",
      "('find', 'book', 'interesting') 1\n",
      "('book', 'interesting', 'good') 2\n",
      "('interesting', 'good', 'mentor') 1\n",
      "('mentor', 'may', 'help.') 1\n",
      "('may', 'help.', 'definitely') 1\n",
      "('help.', 'definitely', 'book') 1\n",
      "('definitely', 'book', 'interesting') 1\n",
      ":::Top n-grams:::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('book', 'interesting', 'good'),\n",
       " ('good', 'mentor', 'may'),\n",
       " ('?', 'hope', 'find'),\n",
       " ('definitely', 'book', 'interesting'),\n",
       " ('find', 'book', 'interesting')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text=u\"Do you know a good mentor may help? Hope you find the book interesting but a good mentor may help. Definitely the book is interesting and good\"\n",
    "\n",
    "# Tokenzing using treebank tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens=tokenizer.tokenize(text)\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "filteredTokens=[t.lower() for t in tokens if t.lower() not in stopWords]\n",
    "\n",
    "trigrams=TrigramCollocationFinder.from_words(filteredTokens)\n",
    "\n",
    "for trigram, freq in trigrams.ngram_fd.items():\n",
    "   print(trigram,freq)\n",
    "\n",
    "print (\":::Top n-grams:::\")\n",
    "trigrams.nbest(TrigramAssocMeasures.raw_freq, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to measure some probabilities. Following code shows probability measurements using  basic MLE (Maximum Likelihood Estimate) and Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'This is an example sentence, example sentence, example sentence example sentence, example sentence'\n",
    "tokens=word_tokenize(text)\n",
    "\n",
    "# First get the frequency distribution of tokens by FreqDist class\n",
    "fdist=FreqDist(token.lower() for token in tokens)\n",
    "# Verify the frequency distribution is correct\n",
    "print (\"Frequency distribution\")\n",
    "print(fdist.items())\n",
    "\n",
    "#Measuring  probabilities\n",
    "print (\"\\nProbability measurements\")\n",
    "mle=MLEProbDist(fdist)\n",
    "print(\"MLE for 'example': \",mle.prob(\"example\"))\n",
    "print(\"MLE for 'this': \",mle.prob(\"this\"))\n",
    "print(\"MLE for 'unknown': \",mle.prob(\"unkown\"))\n",
    "\n",
    "lpl=LaplaceProbDist(fdist)\n",
    "print(\"\\nLaplace for 'example': \",lpl.prob(\"example\"))\n",
    "print(\"Laplace for 'this': \",lpl.prob(\"this\"))\n",
    "print(\"Laplace for 'unknown': \",lpl.prob(\"unkown\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we shall see some examples of trigrams based smoothing using Laplace and Kneser Ney algorthms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have all sentences separately stored in lists to get correct n-grams\n",
    "text = ['I am Sam', ' Sam I am', 'I do not like green eggs and ham']\n",
    "trigramTokens = []\n",
    "\n",
    "for sent in text:\n",
    "    words=word_tokenize(sent)\n",
    "    trigramTokens+=nltk.trigrams(words,pad_left=True, pad_right=True, \n",
    "                    left_pad_symbol='<S>', right_pad_symbol='</S>')\n",
    "\n",
    "### Try to print all of above tri-grams yourself to have an idea of what they look like and\n",
    "### try removing parameters with pad keyword or see the help to understand what is the purpose\n",
    "### of them\n",
    "\n",
    " \n",
    "## Geting an instance of fequency distribution    \n",
    "fdisTri= FreqDist(trigramTokens)\n",
    "\n",
    "## Initializing Kneser Ney Probabilities by using trigrams\n",
    "## Kneser Ney does not work without trigrams\n",
    "knsr=KneserNeyProbDist(fdisTri)\n",
    "print (\"\\n::: Kneser Ney :::\")\n",
    "for tup in knsr.samples():\n",
    "    print (tup,\" = \", knsr.prob(tup))\n",
    "   \n",
    "\n",
    "#Now time for Laplace on trigrams\n",
    "print (\"\\n:::: Laplace :::\")\n",
    "lpl=LaplaceProbDist(fdisTri)\n",
    "for tup in lpl.samples():\n",
    "    print (tup,\" = \", lpl.prob(tup))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we dissect above code, we shall realize that we have created joint probability distributions for N-grams above using different probability measurement methods. For example, see below. The frequency of (<S,<S, I) is 2 (c), total counts of all trigrams is 20 (N) and unique trigrams is 19 (B). The Laplace prob is calculated as: c+1/(N+B) = (2+1)/(20+19)=0.07.\n",
    "This simply gives the joint probabilities (intersection probs) and we need to calculate conditional probability distribution for trigrams by replacing N with the counts of bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdisTri.items())\n",
    "print( \"Total Count\", fdisTri.N())\n",
    "print (\"Distinct Count\", fdisTri.B())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is an example of conditional probability distribution using Laplace smoothing. First we calculate conditional frequency distributions and then we calulate conditional probabilities based on Laplace smoothing. Go through the code and comments to understand it further. In this code, we are also measuring the perplexity of the model by using the equations: \n",
    "\n",
    "Prod= (Product-for-all-trigrams-in-test(P (w3|w2,w1) ) and Power(Prod, -1/(tokens in test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6\n",
      "Items following 'here is': dict_items([('some', 2)])\n",
      "Items following 'is some': dict_items([('repetitive', 2), ('</S>', 1)])\n",
      "P(repetitive|is,some)= 0.3333333333333333\n",
      "P(some|is,some)= 0.1111111111111111\n",
      "P(is|is,some)= 0.1111111111111111\n",
      "P(the|the,the)= 0.16666666666666666\n",
      "\n",
      ":::: Now calcualte perplexity on a test sentence :::\n",
      "P( here |  <S> <S> )= 0.2857142857142857\n",
      "P( is |  <S> here )= 0.2857142857142857\n",
      "P( some |  here is )= 0.375\n",
      "P( </S> |  is some )= 0.2222222222222222\n",
      "P( </S> |  some </S> )= 0.2857142857142857\n",
      "Probability of the test text:  0.001943634596695821\n",
      "Perplexity: 3.485596218940725\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, LaplaceProbDist\n",
    "sent= \"Here is some repetitive and here is some repetitive text and is some\"\n",
    "\n",
    "\n",
    "# get some tokens\n",
    "tokens=[word.lower() for word in word_tokenize(sent)]\n",
    "# get the size of vocabulary\n",
    "vocabSize=set(tokens).__len__()\n",
    "print (\"Vocabulary size:\",vocabSize)\n",
    "\n",
    "# Get trigrams\n",
    "tri = nltk.trigrams(tokens,pad_left=True, pad_right=True, \n",
    "                    left_pad_symbol='<S>', right_pad_symbol='</S>' ) \n",
    "\n",
    "# Measuring conditional frequency distributions\n",
    "cfDist = ConditionalFreqDist(((w1,w2),w3) for w1,w2,w3 in tri)\n",
    "\n",
    "#Above line of code is equivalent to the following three lines of code\n",
    "#for w1,w2,w3 in trigrams:\n",
    "#   condition = (w1,w2)\n",
    "#   cfdist[condition][w3] += 1\n",
    "\n",
    "# Let's verify conditional frequencies for some random w1,w2\n",
    "fDist=cfDist[('here','is')]     \n",
    "print (\"Items following 'here is':\", fDist.items())\n",
    "\n",
    "fDist=cfDist[('is','some')]     \n",
    "print (\"Items following 'is some':\", fDist.items())\n",
    "\n",
    "\n",
    "cpDist = ConditionalProbDist(cfDist, nltk.probability.LaplaceProbDist, bins=vocabSize)\n",
    "\n",
    "\n",
    "\n",
    "print(\"P(repetitive|is,some)=\",cpDist[('is','some')].prob('repetitive'))\n",
    "print(\"P(some|is,some)=\",cpDist[('is','some')].prob('more'))\n",
    "print(\"P(is|is,some)=\",cpDist[('is','some')].prob('is'))\n",
    "print(\"P(the|the,the)=\",cpDist[('the','the')].prob('the'))\n",
    "\n",
    "\n",
    "print (\"\\n:::: Now calcualte perplexity on a test sentence :::\")\n",
    "\n",
    "test = 'here is some'\n",
    "tri=nltk.trigrams(word_tokenize(test),\n",
    "                  pad_left=True, pad_right=True, \n",
    "                  left_pad_symbol='<S>', right_pad_symbol='</S>' ) \n",
    "probTest=1.0 \n",
    "tokenCount=0;\n",
    "for w1,w2,w3 in tri:\n",
    "    pr=cpDist[(w1,w2)].prob(w3)\n",
    "    print (\"P(\",w3,\"| \",w1,w2,\")=\", pr )\n",
    "    probTest=probTest * pr\n",
    "    tokenCount=tokenCount+1\n",
    "\n",
    "\n",
    "print (\"Probability of the test text: \",probTest)\n",
    "perplexity=math.pow(probTest,-(1/tokenCount))\n",
    "print(\"Perplexity:\", math.pow(probTest,-(1/tokenCount)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat the same process as above with Kneser Ney smoothing. Kneser Ney smoothing requires all the words in trigram to be passed to it for each condition (see below). The perplexity of the model on test set with Kneser Ney smoothing is lower than Laplace smoothing. However, there is a problem with the implementation of Kneser Ney in the code of NLTK. It is returning 0 values for any trigram pattern even if the word is already present in the vocabulary which does not seem to satisfy smoothing concept. Also, if you slightly change the training data, it would produce 0 probability and won't be comprable to Laplace smoothing above. Let me know if you find out that NLTK implementation is correct and there is something wrong in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= \"Here is some repetitive and here is some repetitive text and is some\"\n",
    "\n",
    "\n",
    "# get some tokens\n",
    "tokens=[word.lower() for word in word_tokenize(sent)]\n",
    "# get the size of vocabulary\n",
    "vocabSize=set(tokens).__len__()\n",
    "print (\"Vocabulary size:\",vocabSize)\n",
    "\n",
    "# Get trigrams\n",
    "tri = nltk.trigrams(tokens,pad_left=True, pad_right=True, \n",
    "                  left_pad_symbol='<S>', right_pad_symbol='</S>')\n",
    "\n",
    "cfdist2 = ConditionalFreqDist(((w1,w2),(w1,w2,w3)) for w1,w2,w3 in tri)\n",
    "\n",
    "fDist2=cfdist2[('here','is')]     \n",
    "print (\"Items following 'here is':\", fDist2.items())\n",
    "cpdist2 = ConditionalProbDist(cfdist2, nltk.probability.KneserNeyProbDist, bins=vocabSize)\n",
    "\n",
    "print(cpdist2[('is','some')].prob(['is','some','repetitive']))\n",
    "print(cpdist2[('is','some')].prob(['is','some','and']))\n",
    "print(cpdist2[('is','some')].prob(['is','some','the']))\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n:::: Now calcualte perplexity on a test sentence :::\")\n",
    "\n",
    "test = 'here is some'\n",
    "tri=nltk.trigrams(word_tokenize(test),\n",
    "                  pad_left=True, pad_right=True, \n",
    "                  left_pad_symbol='<S>', right_pad_symbol='</S>' ) \n",
    "probTest=1.0 \n",
    "tokenCount=0;\n",
    "for w1,w2,w3 in tri:\n",
    "    pr=cpdist2[(w1,w2)].prob([w1,w2,w3])\n",
    "    print (\"P(\",w3,\"| \",w1,w2,\")=\", pr )\n",
    "    probTest=probTest * pr\n",
    "    tokenCount=tokenCount+1\n",
    "\n",
    "\n",
    "print (\"Probability of the test text: \",probTest)\n",
    "perplexity=math.pow(probTest,-(1/tokenCount))\n",
    "print(\"Perplexity:\", math.pow(probTest,-(1/tokenCount)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some exercises for you:\n",
    "\n",
    "2.2) Implement Laplace smoothing for bigrams and unigrams. Measure the preplexity on the test set and comapre it the result of trigram application.\n",
    "\n",
    "2.3) Implement another equation of perplexity which yileds the same result as the equation used above. Here is the equation in three steps: (a) Logbase2Prob= Sum-for-all-trigrams(log2(P(w3|w1,w2)) (b)Ent=(-1/tokens-in-test)x Logbase2Prob (c) Power(2, (ent) )\n",
    "\n",
    "2.4) Wrtie a program to predict  next words given that you already know some words. THis kind of application, you must have already seen on mobile phone keyboards, in Word processing softwares ot elsewhere. This is very easy to implement, you will need to perform training of your data on some text (as above) and in testing you will predict the next word of a given sequence of words by using trigrams (or bigrams). The initial part of the bigram/trigram will be first few words in your test sequence of words and the last part will be the one that has the maximum probability given the previous parts. If you see the help on the probability distribution on NLTK, there are already functions available to solve this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
