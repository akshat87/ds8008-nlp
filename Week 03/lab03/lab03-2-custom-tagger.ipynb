{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall see how to train our own custom tagger on a corpus of tagged sentences, such as Treebank. There are many different ways to train custom taggers. Some discuss here: http://www.nltk.org/book/ch05.htm. We shall focus on the ones we studied in the text book.\n",
    "\n",
    "Below we load tagged sentences from the Treebank corpus, find out all unique tags and unique words. The unique tags will become states in HMM and the unique words will be observation symbols in HMM. Let's analyze this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "\n",
    "\n",
    "# Training sentences extracted from Treebank corpus\n",
    "#Let's take 100 sentences only\n",
    "trainData = treebank.tagged_sents()[:100]\n",
    "\n",
    "## Extract distinct words(observations) and tags (states)\n",
    "allStates=set() # tags in our case\n",
    "observationSymbols=set() #Vocabulary in our case\n",
    "for t in trainData:\n",
    "    for (word,tag) in t:\n",
    "        allStates.add(tag)\n",
    "        observationSymbols.add(word)\n",
    "\n",
    "allStates=list(allStates)\n",
    "observationSymbols=list(observationSymbols) \n",
    "\n",
    "print (\"Total States (tags): \",len(allStates))\n",
    "print(\"Total Observation Symbols (Vocab): \",len(observationSymbols))\n",
    "\n",
    "print(\"********** A sample sentence **********\")\n",
    "print (trainData[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an HMM model by training it on the above extracted data.  There are two ways to train an HMM. A supervised method and an unsupervised emthod. In a suspervised method we know the observation sequences and their states  in the training data and we estimate probabilities using a maximum likelihood estimate (MLE). In unsupervised method we use Baulm Welch algorithm for training an HMM on the data of only observation sequences (states are not present in the data). It learns probabilities itself by iterating over data. \n",
    "\n",
    "We shall use the supervise method as we have both word (observations) sequences  and tags (states). In addition to training, we are also testing the model on a test sentence to generate POS (part-of-speech) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import MLEProbDist\n",
    "from nltk.tag import hmm\n",
    "\n",
    "smoothingFunction = lambda fdist, bins: MLEProbDist(fdist,bins)\n",
    "\n",
    "# And train with the data\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states=allStates,symbols=observationSymbols)\n",
    "tagger = trainer.train_supervised(trainData,estimator=smoothingFunction)\n",
    "print (tagger)\n",
    "test=\"Cigarette has caused a high percentage of cancer.\"\n",
    "print (tagger.tag(nltk.word_tokenize(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at the output above and compare the tags with the tags in a sentence of the original Treebank corpus, printed in the step before the previous step.\n",
    "\n",
    "All the tags are incorrect. We have the same JJR tag for everything. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fix this by using Laplace smoothing, see below code. When you run the code below, you'll find out that most of the tags are correct by comparing with the original tags shown above. However, there are some tags which are incorrect. Can you determine whihc one? The accuracy for the test sentence  is approximately 70-80% (just compare how many are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import LaplaceProbDist\n",
    "test=\"Cigarette has caused a high percentage of cancer.\"\n",
    "\n",
    "# Prints the basic data about the tagger\n",
    "\n",
    "smoothingFunction = lambda fdist, bins: LaplaceProbDist(fdist,bins)\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states=allStates,symbols=observationSymbols)\n",
    "tagger = trainer.train_supervised(trainData,estimator=smoothingFunction)\n",
    "print(tagger)\n",
    "print (tagger.tag(nltk.word_tokenize(test)))\n",
    "#print (tagger.tag(\"Most parts of speech can\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve our accuracy by training on a larger number of records. With approximately 2000 records, we are almost there. (See below). At the end, we shall also see the comparison with NLTK's trained tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import hmm\n",
    "\n",
    "\n",
    "trainData2 = treebank.tagged_sents()[:2000]\n",
    "\n",
    "allStates2=set()\n",
    "observationSymbols2=set() #Vocabulary in our case\n",
    "for t in trainData2:\n",
    "    for (word,tag) in t:\n",
    "        allStates2.add(tag)\n",
    "        observationSymbols2.add(word)\n",
    "\n",
    "allStates2=list(allStates2)\n",
    "observationSymbols2=list(observationSymbols2) \n",
    "        \n",
    "print (\"Total States (tags): \",len(allStates2))\n",
    "print(\"Total Observation Symbols (Vocab): \",len(observationSymbols2))\n",
    "\n",
    "smoothingFunction = lambda fdist, bins: LaplaceProbDist(fdist,bins)\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states=allStates2,\\\n",
    "                                       symbols=observationSymbols2)\n",
    "tagger = trainer.train_supervised(trainData2,estimator=smoothingFunction)\n",
    "\n",
    "print(tagger)\n",
    "\n",
    "test=\"Cigarette has caused a high percentage of cancer.\"\n",
    "\n",
    "print (\"\\n**** Test: Ouptut of HMM tagger based on Viterbi algorithm ****\")\n",
    "print (tagger.tag(nltk.word_tokenize(test)))\n",
    "\n",
    "print (\"\\nComparsion with NLTK's trained tagger*****\")\n",
    "print( nltk.pos_tag(nltk.word_tokenize(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.2: Train an HMM model on the sentences of Brown corpus. Divide the data set (tagged sentences) into 70% for training and 30% for testing. Find out the accuracy of your trained HMM model on the sentences in test data. To determine accuracy, you will need to compare the predicted tags of words with the original tags of words in the test data. \n",
    "\n",
    "\n",
    "Exercise 3.3: Use the same test data as in Exercise 3.2, use NLTK's tagger to predict the tags and determine accuracy of prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
