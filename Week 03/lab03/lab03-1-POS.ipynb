{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn parto-of-speech (POS) tagging and its fundamental applciation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#Run the following and download \"Everything used in the NLTK book\"\n",
    "nltk.download()\n",
    "# In case, you recive error on tagging, download: \n",
    "#nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of speech are also known as word classes or lexical categories. Tagging POS is usually the second step in NLP pipeline after tokenization.\n",
    "An example of the part-of-speech tagging is shown below. The default tagset used by nltk tagger is Penn Treebank tagset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text1=nltk.word_tokenize(\"Most parts of speech can be divided into sub-classes.\")\n",
    "print(nltk.pos_tag(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of Penn Treebank tagsets is present here:\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "Alternately, you can also you following help commands to find out abut tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#Run this to look up individual tags\n",
    "\n",
    "nltk.help.upenn_tagset(\"JJS\")\n",
    "\n",
    "# Uncomment and run the below command to get the list of all the tags\n",
    "#print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see words and tags in two corporus: Treebank corpus and Brown corpus. There are other corpus present in NLTK too (www.nltk.org/book) and each corpus may have different notations for tags. There is an option in NLTK to map different tag symobls to the same universal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.treebank.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the occurrence of some common tags in the Treebank corpus with Treebank tagset and universal tagset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "treebank_tagged = treebank.tagged_words()\n",
    "tag = nltk.FreqDist(tag for (word, tag) in treebank_tagged)\n",
    "tag.most_common(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "treebank_tagged = treebank.tagged_words(tagset='universal')\n",
    "tag = nltk.FreqDist(tag for (word, tag) in treebank_tagged)\n",
    "tag.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient way to identify tags of many senetences is using pos_tag_sents function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text=\"Most parts of speech can be divided into sub-classes. Prepositions can be divided \\\n",
    "into prepositions of time, prepositions of place etc. Nouns can be divided into proper \\\n",
    "nouns, common nouns, concrete nouns etc.\"\n",
    "\n",
    "sents=nltk.sent_tokenize(text)\n",
    "for i,s in enumerate(sents):\n",
    "    sents[i]=nltk.word_tokenize(s)\n",
    "#for sent in sents:\n",
    "print (nltk.pos_tag_sents(sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking refers to a proces of performing entity detection. It is used to identify  multiple sequences of tokens in a sentence as one chunk.\n",
    "For example, International Business Machines is one entity with a sequence of words. If we perform simple tokenization, we would get different words. If we want to extract  \"International Business Machines\" as one token then chunking can help us. This kind of token extraction can be helpful information retireval, text classfication, text clustering, document sumamrization and etc. For example, suppose a document retireval application that will return different documents containing interantional, business and machine keywords as compared to documents containing International Business Machine as one word.\n",
    "\n",
    "A simple way to  design a chunker is to use a chunk grammar. A chunk grammar holds the rules of how chunking should be done. Chunk grammar simply contains regular expression of tags of parts of speech. Below are some examples of chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "taggedWords=[(\"financial\",\"NN\"),(\"year\",\"NN\"),(\"account\",\"NN\"),(\"summary\",\"NN\")]\n",
    "# NP-CHUNk is the name we assigned to the token and anything inside curly\n",
    "# brackets is the chunk expression. Here we are extracting one or more \n",
    "#occurrences of NN (Noun Singular) as one token.\n",
    "chunkGrammar=\"NP-CHUNK: {<NN>+}\"\n",
    "find = nltk.RegexpParser(chunkGrammar)\n",
    "print(find.parse(taggedWords)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=nltk.word_tokenize(\"International Business Machine (IBM) started \\\n",
    "its operation more than 100 years ago.\")\n",
    "tagSent=nltk.pos_tag(text1)\n",
    "print (\"Tagged sent: \", tagSent)\n",
    "\n",
    "#Get chunks with repetitive occurrences of \n",
    "#singular noun <NN> or Proper Noun Singular <NNP>+\n",
    "chunkGram=\"NP-CHUNK: {<NN>+|<NNP>+}\"\n",
    "find = nltk.RegexpParser(chunkGram)\n",
    "chunkTree=find.parse(tagSent)\n",
    "\n",
    "    \n",
    "print(\"\\n:******* All the chunked noun phrases *******\")\n",
    "for subtree in chunkTree.subtrees():\n",
    "    if subtree.label() == 'NP-CHUNK':\n",
    "        finalChunk=\"\"\n",
    "        for (w,tag) in subtree.leaves():\n",
    "            finalChunk=finalChunk + \" \" + w\n",
    "        print (finalChunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on Chunking is  here (from Section 2.1 to Section 2.4): http://www.nltk.org/book/ch07.html.\n",
    "\n",
    "Exercise 3.1: Your task is to get some text from Wikepedia (just copy paste in a text document or in a variable) and extract three different chunks of your choice: e.g., co-ocurrences of adjectives and nouns, co-occurences of determiner, adjectives and nouns, extractions of all types of nouns, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
