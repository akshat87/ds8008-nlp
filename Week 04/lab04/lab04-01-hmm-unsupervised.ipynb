{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we shall learn the unsupervise training of HMM.\n",
    "\n",
    "Let's review the code of supervise training of HMM from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import hmm\n",
    "\n",
    "\n",
    "trainData2 = treebank.tagged_sents()[:2000]\n",
    "\n",
    "#Extract distinct tags and words (vocab) from our dataset\n",
    "allStates2=set() # tags in our casse\n",
    "observationSymbols2=set() #Vocabulary in our case\n",
    "for t in trainData2:\n",
    "    for (word,tag) in t:\n",
    "        allStates2.add(tag)\n",
    "        observationSymbols2.add(word)\n",
    "\n",
    "allStates2=list(allStates2)\n",
    "observationSymbols2=list(observationSymbols2) \n",
    "        \n",
    "print (\"Total States (tags): \",len(allStates2))\n",
    "print(\"Total Observation Symbols (Vocab): \",len(observationSymbols2))\n",
    "\n",
    "# Laplace prob distribution for somoothing of probabilities\n",
    "smoothingFunction = lambda fdist, bins: LaplaceProbDist(fdist,bins)\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer(states=allStates2,\n",
    "                                       symbols=observationSymbols2)\n",
    "supervisedHmm = trainer.train_supervised(trainData2,estimator=smoothingFunction)\n",
    "\n",
    "#print(tagger)\n",
    "\n",
    "#test=\"Cigarette has caused a high percentage of cancer.\"\n",
    "test=\"Israel television rejected a skit by comedian Tuvia Tzafir that attacked public apathy by depicting an Israeli family watching TV while a fire raged outside .\"\n",
    "print (\"\\n**** Test: Ouptut of HMM tagger based on Viterbi algorithm ****\")\n",
    "print (supervisedHmm.tag(nltk.word_tokenize(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we have additional data other than the Treebank tagged data. This new data is not tagged with parts of speech. We have already built a supervised HMM on the tagged data as above, and we wish to update our trained model by using the new unlabeled data. This data is present in the file sents-01.txt. Open it in a Notepad or elsewhere to see its contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get states and symbols from the previously trained model\n",
    "theStates=supervisedHmm._states\n",
    "theSymbols=set(supervisedHmm._symbols)\n",
    "\n",
    "# Load the training sentences from the file\n",
    "trainData=[]\n",
    "file=open(\"sents-01.txt\")\n",
    "for line in file:\n",
    "    trainData.append(line)\n",
    "\n",
    "#We have to convert training sentences into the tokenize words of the \n",
    "# following form: [ [(w1,None)...(wk,None)],[(w1,None)...(wk,None)]] \n",
    "# As there is no tag associated with the words, NLTK requires us to\n",
    "# have None keyword instead of tag beside each word. The following code\n",
    "#performs this operation. In addition, the code also updates the symbols\n",
    "# (words) of the previous model with the new symbols (words). In other\n",
    "# words, we need to update the vocabulary in advance to tell HMM that \n",
    "# these new words we shall see during training.\n",
    "tokenSeq=[]\n",
    "for sent in trainData:\n",
    "    words=nltk.word_tokenize(sent)\n",
    "    wordTagList=[]\n",
    "    for word in words:\n",
    "        theSymbols.add(word) ## we have to add new vocabulary to our symbols list\n",
    "        wordTagList.append((word,None))\n",
    "    tokenSeq.append(wordTagList)\n",
    "\n",
    "# Since HMM is nothing but a bunch of matrices, we need to extract all\n",
    "#the matrices from the previous model and initialize a new model with our\n",
    "# updated symbols. Transition matrix represents the transititon probs. from\n",
    "# one state (tag) to another and output matrix and priors represent what?\n",
    "# I leave that to you to explore this from the lecture.\n",
    "initModel=hmm.HiddenMarkovModelTagger(states=theStates,\n",
    "                                        symbols=list(theSymbols),\n",
    "                                       transitions=supervisedHmm._transitions, \n",
    "                                        outputs=supervisedHmm._outputs,\n",
    "                                        priors=supervisedHmm._priors)\n",
    "print (initModel)\n",
    "# Next we need to initalize the trainer model, just like the sueprvise HMM\n",
    "# (Note this is the requirement in NLTK to intialize things like this, other\n",
    "#libraries may have different steps and you can create your own library\n",
    "#with different steps.)\n",
    "unSuperTrainer= hmm.HiddenMarkovModelTrainer(states=theStates,\n",
    "                                        symbols=list(theSymbols))\n",
    "\n",
    "# Finally, we shall now use Baum Welch for training on new unlabeled data\n",
    "# by using our initial supervise model as the starting point.\n",
    "unsupervisedHmm=unSuperTrainer.train_unsupervised(unlabeled_sequences=tokenSeq,\n",
    "                                         max_iterations=50,model=initModel)\n",
    "\n",
    "print(unsupervisedHmm)\n",
    "print (\"Training finished.....\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of supervise and unsupervise HMM models on the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test=\"Israel television rejected a skit by comedian Tuvia Tzafir that attacked public apathy by depicting an Israeli family watching TV while a fire raged outside .\"\n",
    "print (\"\\n**** Test: Ouptut of HMM tagger based on Viterbi algorithm ****\")\n",
    "print (\"Unsupervised Result: \",unsupervisedHmm.tag(nltk.word_tokenize(test)))\n",
    "print (\"Supervised Result:  \",supervisedHmm.tag(nltk.word_tokenize(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1\n",
    "\n",
    "The difference in improvement does not seem to be much between unsupervised HMM and supervised HMM. There are several reasons.There is not sufficient data for training an unsupervised HMM and also the loss of tags for training can result in the loss of information. Nonetheless, your task is to pick 5 different sentences from the treebank corpus, use unsupervise and supervise HMM models trained above for predictions of tags and then compare the results to determine which one produces better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better use of unsupervise HMM is classfication or anomaly detection. In the following example, we shall train two HMMs  on the training setnences of sent-01 (unlabeled data) and sent-02 (unlabled data).  We shall then try to predict whether any unknown sentence belongs to one of the training dataset or not by using the log probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "# All symbols\n",
    "allSymbols=set()\n",
    "\n",
    "# Function to convert a file to training sentences\n",
    "def fileToTrainData(fileName):\n",
    "    trainSent=[]\n",
    "    file=open(fileName)\n",
    "    for line in file:\n",
    "        trainSent.append(line)\n",
    "    return trainSent\n",
    "\n",
    "# Function to convert training setences to token sequences\n",
    "def getTokenSeq(trainSents):\n",
    "    tokenSeq=[]\n",
    "    for sent in trainSents:\n",
    "        words=nltk.word_tokenize(sent)\n",
    "        wordTagList=[]\n",
    "        for word in words:\n",
    "            allSymbols.add(word)## creating a vocabulary set\n",
    "            wordTagList.append((word,None))\n",
    "        tokenSeq.append(wordTagList)\n",
    "    return tokenSeq\n",
    "\n",
    "\n",
    "# Convert training files to token sequences and determine total vocabulary\n",
    "# (symbols) as well (see above functions)\n",
    "tokenSeq01=getTokenSeq(fileToTrainData(\"sents-01.txt\"))\n",
    "tokenSeq02=getTokenSeq(fileToTrainData(\"sents-02.txt\"))\n",
    "\n",
    "# We are randomly picking 10 states for our experiments\n",
    "totStates=10\n",
    "states=[s for s in range(totStates)]\n",
    "totSymbols=len(allSymbols)\n",
    "\n",
    "print (\"Total symbols: \",totSymbols)\n",
    "print (\"States: \",states)\n",
    "\n",
    "# We need ot intialize HMM with some random values of \n",
    "#transition matrix (A),  ouptut matrix (B) and prior probability matrix (pi)\n",
    "# See the lecture notes to understand their dimensions. Feel free to \n",
    "#print the matrices to understand them. Note that the sum of the rows of \n",
    "# each matrix needs to be one as each cell represents a probability value.\n",
    "\n",
    "# Matrix A\n",
    "# create a matrix with random values\n",
    "randArr=np.random.rand(totStates,totStates) \n",
    "#making sum of row 1: dividing by total sum of rows\n",
    "A=randArr/randArr.sum(axis=1, keepdims=True) \n",
    "\n",
    "#Matrix B\n",
    "randArr=np.random.rand(totStates,totSymbols)\n",
    "B=randArr/randArr.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Vector Pi (one row only)\n",
    "randVector=np.random.rand(totStates)\n",
    "pi=randVector/randVector.sum(axis=0, keepdims=True)\n",
    "\n",
    "## Validate that the sum is one\n",
    "print(pi)\n",
    "print (pi.sum(0))\n",
    "\n",
    "#print (A)\n",
    "#print (B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now train the unsupervised HMM on the training sequences extracted in the previous step. At the end of the following example you will see the log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hTagger01=hmm._create_hmm_tagger(states,list(allSymbols) , A, B, pi)\n",
    "trainer01 = hmm.HiddenMarkovModelTrainer(states, list(allSymbols))\n",
    "trainer01Hmm = trainer01.train_unsupervised(tokenSeq01, model=hTagger01,\n",
    "                                            max_iterations=50)\n",
    "\n",
    "test=\"Israel television rejected a skit by comedian Tuvia Tzafir.\"\n",
    "print (\"\\n**** Test: Log Probability ****\")\n",
    "tokensTest=[(w,None) for w in nltk.word_tokenize(test)]\n",
    "print (trainer01Hmm.log_probability(tokensTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2\n",
    "\n",
    "We have trained the first HMM on the training data (sent-01). You will have to repeat the process on the sent-02 data. Follow the same process as above. Once the second HMM is trained, measure the log probabilties of the following sentences by using both the HMM models of sent-01 and sent-02. \n",
    "<code>\n",
    "test1=\"An art exhibit in Arab east Jerusalem was a series of portraits\"\n",
    "test2=\"Rome is in Lazio province.\"\n",
    "test3=\"W. Dale Nelson covers the White House for The Associated Press .\"\n",
    "test4=\"The news agency in Umbria\"\n",
    "</code>\n",
    "\n",
    "Whichever model genrates a bigger log probability (note in negative number -4 > -10) the test sentence will belong to that particular dataset.  \n",
    "\n",
    "# Exercise 4.3\n",
    "In the previous exercise (4.2), we performed random intialization of matrices for training two HMMs. Repeat the experiments 5 times, every time with different intialization of random values and compare the results on the four test sentences of Exercise 4.2.\n",
    "\n",
    "# Exercise 4.4\n",
    "Repeat the exepriments of Exercise 4.2 by changing the number of states to 5, 15 and 20. For each number of states perform testing on the four test sentences of Exercise 4.2 and compare the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
